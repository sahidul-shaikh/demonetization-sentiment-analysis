{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\n\nfrom textblob import TextBlob","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading and understanding the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/demonetization-in-india-twitter-data/demonetization-tweets.csv',encoding='ISO-8859-1')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking is there is any missing valaues in tweet\ndf['text'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning the tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df['text']\ndf = pd.DataFrame({'tweet':df})\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing few characters \ndf['cleaned_tweet'] = df['tweet'].replace(r'\\'|\\\"|\\,|\\.|\\?|\\+|\\-|\\/|\\=|\\(|\\)|\\n|\"', '', regex=True)\n# Replacing few double spaces with single space\ndf['cleaned_tweet'] = df['cleaned_tweet'].replace(\"  \", \" \")\n\n# remove emoticons form the tweets\ndf['cleaned_tweet'] = df['cleaned_tweet'].replace(r'<ed>','', regex = True)\ndf['cleaned_tweet'] = df['cleaned_tweet'].replace(r'\\B<U+.*>|<U+.*>\\B|<U+.*>','', regex = True)\n\n# convert tweets to lowercase\ndf['cleaned_tweet'] = df['cleaned_tweet'].str.lower()\n    \n#remove user mentions\ndf['cleaned_tweet'] = df['cleaned_tweet'].replace(r'^(@\\w+)',\"\", regex=True)\n    \n#remove 'rt' in the beginning\ndf['cleaned_tweet'] = df['cleaned_tweet'].replace(r'^(rt @)',\"\", regex=True)\n    \n#remove_symbols\ndf['cleaned_tweet'] = df['cleaned_tweet'].replace(r'[^a-zA-Z0-9]', \" \", regex=True)\n\n#remove punctuations \ndf['cleaned_tweet'] = df['cleaned_tweet'].replace(r'[[]!\"#$%\\'()\\*+,-./:;<=>?^_`{|}]+',\"\", regex = True)\n\n#remove_URL(x):\ndf['cleaned_tweet'] = df['cleaned_tweet'].replace(r'https.*$', \"\", regex = True)\n\n#remove 'amp' in the text\ndf['cleaned_tweet'] = df['cleaned_tweet'].replace(r'amp',\"\", regex = True)\n\n#remove words of length 1 or 2 \ndf['cleaned_tweet'] = df['cleaned_tweet'].replace(r'\\b[a-zA-Z]{1,2}\\b','', regex=True)\n\n#remove extra spaces in the tweet\ndf['cleaned_tweet'] = df['cleaned_tweet'].replace(r'^\\s+|\\s+$',\" \", regex=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have the cleaned_tweet. But the stop words are still present. We can use this cleaned_tweet forcreating phrases and ranking the phrases from the tweets.\n\nWe will also remove the stop words and name the column as fully_cleaned_tweet. This will be used for clustering the sentiments."},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of words to remove\nwords_to_remove = [\"ax\",\"i\",\"you\",\"edu\",\"s\",\"t\",\"m\",\"subject\",\"can\",\"lines\",\"re\",\"what\", \"there\",\"all\",\"we\",\n                \"one\",\"the\",\"a\",\"an\",\"of\",\"or\",\"in\",\"for\",\"by\",\"on\",\"but\",\"is\",\"in\",\"a\",\"not\",\"with\",\"as\",\n                \"was\",\"if\",\"they\",\"are\",\"this\",\"and\",\"it\",\"have\",\"has\",\"from\",\"at\",\"my\",\"be\",\"by\",\"not\",\"that\",\"to\",\n                \"from\",\"com\",\"org\",\"like\",\"likes\",\"so\",\"said\",\"from\",\"what\",\"told\",\"over\",\"more\",\"other\",\n                \"have\",\"last\",\"with\",\"this\",\"that\",\"such\",\"when\",\"been\",\"says\",\"will\",\"also\",\"where\",\"why\",\n                \"would\",\"today\", \"in\", \"on\", \"you\", \"r\", \"d\", \"u\", \"hw\",\"wat\", \"oly\", \"s\", \"b\", \"ht\", \n                \"rt\", \"p\",\"the\",\"th\", \"n\", \"was\", \"via\"]\n\n#remove stopwords and words_to_remove\nstop_words = set(stopwords.words('english'))\nmystopwords = [stop_words, words_to_remove]\n\ndf['fully_cleaned_tweet'] = df['cleaned_tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in mystopwords]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assigning sentiment for each tweet which ranges between -1 to +1\n# -1.0 is a negative polarity \n# 1.0 is a positive\n# 0 is a neutral polarity\ndf['sentiment'] = df['fully_cleaned_tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vectorize the tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating tokens for each tweet\ndf['tokenized_tweet'] = df['fully_cleaned_tweet'].apply(word_tokenize)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if a word has a digit, remove that word\ndf['tokenized_tweet'] = df['tokenized_tweet'].apply(lambda x: [y for y in x if not any(c.isdigit() for c in y)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set values for various parameters\nnum_features = 100    # Word vector dimensionality                      \nmin_word_count = 1   # Minimum word count                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 10          # Context window size   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize and train the model\nfrom gensim.models import word2vec\n\nmodel = word2vec.Word2Vec(df['tokenized_tweet'], \n                          workers=num_workers, \n                          size=num_features, \n                          min_count = min_word_count, \n                          window = context)\n\n# init_sims will make the model much more memory-efficient.\nmodel.init_sims(replace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find vector corresponding to each tweet\nTake the average of all word vectors in a tweet. This way we will find the vector for each tweet."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nvocab = list(model.wv.vocab)\n\ndef sentence_vector(sentence, model):\n    nwords = 0\n    featureV = np.zeros(100, dtype=\"float32\")\n    for word in sentence:\n        featureV = np.add(featureV, model[word])\n        nwords = nwords + 1\n        \n    featureV = np.divide(featureV, nwords)\n    return featureV\n\ntweet_vector = df['tokenized_tweet'].apply(lambda x: sentence_vector(x, model))  \n\ntweet_vector = tweet_vector.apply(pd.Series)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ntweet_vector = scaler.fit_transform(tweet_vector)\ntweet_vector = pd.DataFrame(tweet_vector)\ntweet_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Add sentiment to the tweet vector"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scale the 'sentiment' vector, as the sentiment varies from -1 to +1\ndef sentiment(x):\n    if x < 0.04:\n        return 0\n    elif x > 0.04:\n        return 1\n    else:\n        return 0.5\n\n# Adding sentiment to the 100th dimension\ntweet_vector[100] = df['sentiment'].apply(lambda x: sentiment(x))\n\ntweet_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Updating the 'sentiment' column in df also\ndf['sentiment'] = tweet_vector[100]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cluster the narratives [= opinions + expressions]"},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of indices of missing value rows\nmissing_row_indices = tweet_vector[tweet_vector.isnull().any(axis=1)].index.to_list()\nprint(missing_row_indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the rows with any missing values\ntweet_vector = tweet_vector.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the same rows dropped in tweet_vector\ndf = df.drop(missing_row_indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-Means clustering of the tweet vectors\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, silhouette_samples\n\nrange_n_clusters = [3, 4, 5, 6, 7, 8, 9, 10]\nX = tweet_vector\nn_best_clusters = 0\nsilhouette_best = 0\n\nfor n_clusters in range_n_clusters:\n    \n    # Initialize the clusterer with n_clusters value and a random generator\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n                                      \n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n    \n    if silhouette_avg > silhouette_best:\n        silhouette_best = silhouette_avg\n        n_best_clusters = n_clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best number of cluster\nn_best_clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clustering with n_best_clusters\nclusterer = KMeans(n_clusters= n_best_clusters , random_state=10)\ncluster_labels = clusterer.fit_predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters = np.unique(cluster_labels)  \nprint(clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Array of tweets, the corresponding cluster number, sentiment\nfinaldf = pd.DataFrame({'cl_num': cluster_labels,'fully_cleaned_tweet': df['fully_cleaned_tweet'], 'cleaned_tweet': df['cleaned_tweet'], 'tweet': df['tweet'],'sentiment': df['sentiment']})\nfinaldf = finaldf.sort_values(by=['cl_num'])\nfinaldf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding cluster numbers to df as well\ndf['cl_num'] = cluster_labels\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfOrdered = pd.DataFrame(df)\n\n#Compute how many times a tweet has been 'retweeted' - that is, how many rows in dfOrdered are identical\ndfOrdered['tokenized_tweet'] = dfOrdered['tokenized_tweet'].apply(tuple)\ndfUnique = dfOrdered.groupby(['tweet', 'cleaned_tweet', 'fully_cleaned_tweet', 'sentiment','tokenized_tweet', 'cl_num']).size().reset_index(name=\"freq\")\ndfUnique = dfUnique.sort_values(by=['cl_num'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfUnique['tokenized_tweet'] = dfUnique['tokenized_tweet'].apply(list)\ndfOrdered['tokenized_tweet'] = dfOrdered['tokenized_tweet'].apply(list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see that there are around 5000 unique tweets\ndfUnique.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate abstraction and expression for each narrative\n\n\n* Abstraction: What the opinion is about, for e.g. an opinion on demonetisation can be 'about a topic' such as 'Digital India', corruption, PM Modi, etc.\n* Expression: The 'sentiment' of the opinion, i.e. positive, negative or neutral.\n\nEach cluster represents a narrative. In other words, people start supporting other people having similar opinions, and as a result, opinions turn into narratives."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Store all tweets corresponding to each cluster in a file\nfor i in clusters:\n    with open('./tweets_Cluster_'+str(i)+'.txt','w') as out:\n        y = ''\n        for x in dfUnique['fully_cleaned_tweet'][dfUnique.cl_num == i]:    \n            y = y + x + '. '\n        out.write(y)\n        out.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#A combination of (Noun, adjective, cardinal number, foreign word and Verb) are being extracted now\n#Extract chunks matching pattern. Patterns are:\n#1) Noun phrase (2 or more nouns occurring together. Ex United states of America, Abdul Kalam etc)\n#2) Number followed by Noun (Ex: 28 Terrorists, 45th President)\n#3) Adjective followed by Noun (Ex: Economic impact, beautiful inauguration)\n#4) Foreign word (Ex: Jallikattu, Narendra modi)\n#5) Noun followed by Verb (Ex: Terrorists arrested)\n#And a combination of all 5\n        \nimport re\nimport nltk\n\nphrases = pd.DataFrame({'extracted_phrases': [], 'cluster_num': []})\n\n\nA = '(CD|JJ)/\\w+\\s'  #cd or jj\nB = '(NN|NNS|NNP|NNPS)/\\w+\\s'  #nouns\nC = '(VB|VBD|VBG|VBN|VBP|VBZ)/\\w+\\s' #verbs\nD = 'FW/\\w+\\s'  #foreign word\npatterns = ['('+A+B+')+', '('+D+B+')+','('+D+')+', '('+B+')+', '('+D+A+B+')+', \n           '('+B+C+')+', '('+D+B+C+')+', '('+B+A+B+')+', '('+B+B+C+')+'] \n\n\ndef extract_phrases(tag1, tag2, sentences):\n    extract_phrase = []\n    for sentence in sentences:\n        phrase = []\n        next_word = 0\n        for word, pos in nltk.pos_tag(nltk.word_tokenize(sentence)):\n            if next_word == 1:\n                next_word = 0\n                if pos == tag2:\n                    extract_phrase = np.append(extract_phrase,phrase + ' ' + word) \n            \n            if pos == tag1:\n                next_word = 1\n                phrase = word\n    return extract_phrase\n\nfor i in clusters:\n    File = open('./tweets_Cluster_'+str(i)+'.txt', 'r') #open file\n    lines = File.read() #read all lines\n    sentences = nltk.sent_tokenize(lines) #tokenize sentences\n\n    for sentence in sentences: \n        f = nltk.pos_tag(nltk.word_tokenize(sentence))\n        tag_seq = []\n        for word, pos in f:\n            tag_seq.append(pos+'/'+ word)\n        X = \" \".join(tag_seq)\n\n        phrase = []\n        for j in range(len(patterns)):\n            if re.search(patterns[j], X):\n                phrase.append(' '.join([word.split('/')[1] for word in re.search(patterns[j], X).group(0).split()]))\n    \n        k = pd.DataFrame({'extracted_phrases': np.unique(phrase), 'cluster_num': int(i)})\n    \n        phrases = pd.concat([phrases,k], ignore_index = True)\n\nprint(phrases)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Keeping the largest phrase"},{"metadata":{"trusted":true},"cell_type":"code","source":"#For each phrase identified replace all the substrings by the largest phrase \n#Ex: lakh looted,40 lakh looted and Rs 40 lakh looted, replace all by single largest phrase - Rs 40 lakh looted \n#i.e. instead of 3 different phrases, there will be only one large phrase\n\nphrases_final = pd.DataFrame({'extracted_phrases': [], 'cluster_num': []})\nfor i in clusters:\n    phrases_for_each_cluster = []\n    cluster_phrases = phrases['extracted_phrases'][phrases.cluster_num == i]\n    cluster_phrases = np.unique(np.array(cluster_phrases))\n    for j in range(len(cluster_phrases)):\n        \n        phrase = cluster_phrases[j]\n        updated_cluster_phrases = np.delete((cluster_phrases), j)\n        if any(phrase in phr for phr in updated_cluster_phrases): \n            'y'\n        else: \n            #considering phrases of length greater than 1 only\n            if (len(phrase.split(' '))) > 1:\n                phrases_for_each_cluster.append(phrase)\n    k = pd.DataFrame({'extracted_phrases': phrases_for_each_cluster, 'cluster_num': int(i) })\n    \n    phrases_final = pd.concat([phrases_final,k], ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phrases_final","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate TF-IDF score "},{"metadata":{},"cell_type":"markdown","source":"## For each phrase in each cluster, calculate term frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfUnique.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Term-frequency : For each cluster, calculate the number of times a given phrase occur in the tweets of that cluster\n\nphrases_final['term_freq'] = len(phrases_final)*[0]\n\nfor i in clusters:\n    for phrase in phrases_final['extracted_phrases'][phrases_final.cluster_num == i]:\n        tweets = dfUnique['fully_cleaned_tweet'][dfUnique.cl_num == i]\n        for tweet in tweets:\n            if phrase in tweet:\n                phrases_final['term_freq'][(phrases_final.extracted_phrases == phrase) & (phrases_final.cluster_num == i)] = phrases_final['term_freq'][(phrases_final.extracted_phrases == phrase) & (phrases_final.cluster_num == i)] + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For each phrase in each cluster, calculate document frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Document-frequency\nphrases_final['doc_freq'] = len(phrases_final)*[0]\n\n\n# for each phrase, compute the number of clusters that Sphrase occurs in\nfor phrase in phrases_final['extracted_phrases']:\n    for i in clusters:\n        all_tweets = ''\n        for tweet in dfUnique['fully_cleaned_tweet'][dfUnique.cl_num == i]:\n            all_tweets = all_tweets + tweet + '. ' \n        if phrase in all_tweets:\n            phrases_final['doc_freq'][(phrases_final.extracted_phrases == phrase) & (phrases_final.cluster_num == i)] = phrases_final['doc_freq'][(phrases_final.extracted_phrases == phrase) & (phrases_final.cluster_num == i)] + 1\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate IDF\nimport math\nphrases_final['doc_freq'] = phrases_final['doc_freq'].apply(lambda x: math.log10(n_best_clusters/(x)) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For each phrase in each cluster, calculate tf-idf"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate TF-IDF\n# TF X IDF\nphrases_final['tf-idf'] = phrases_final['term_freq']*phrases_final['doc_freq']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phrases_final","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## For each cluster find top few phrases and respective sentiment"},{"metadata":{"trusted":true},"cell_type":"code","source":"phrases_final['diff_tf-idf'] = len(phrases_final)*[0]\n\nnarrative = pd.DataFrame({'cl_num': [], 'abstraction': []})\nfor i in clusters: \n    # arrange in descending order of tf-idf score\n    phrases_final = phrases_final.sort_values(['cluster_num','tf-idf'], ascending=[1,0])\n    \n    #Break this distribution at a point where the difference between any consecutive phrases is maximum\n    #difference between consecutive values of tf-idf \n    phrases_final['diff_tf-idf'][phrases_final.cluster_num == i] = abs(phrases_final['tf-idf'][phrases_final.cluster_num == i] - phrases_final['tf-idf'][phrases_final.cluster_num == i].shift(1))\n\n    #The last value for each cluster will be 'NaN'. Replacing it with '0'. \n    phrases_final = phrases_final.fillna(0)\n    \n    phrases_final = phrases_final.reset_index(drop = True) #to avoid old index being added as a new column\n    if len(phrases_final[phrases_final.cluster_num == i]) != 0:\n        \n        #index corresponding to the highest difference\n \n        ind = (phrases_final['diff_tf-idf'][phrases_final.cluster_num == i]).idxmax()\n        \n        abstract = phrases_final['extracted_phrases'][:ind+1][phrases_final.cluster_num == i]\n    \n    \n        #store the abstraction corresponding to each cluster\n        k = pd.DataFrame({'cl_num': int(i), 'abstraction': abstract})\n        narrative = pd.concat([narrative,k], ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"narrative","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Assigning polarity based on the sentiment for each tweet 2=negative, 1=positive, 3=neutral\ndfUnique['polarity'] = np.NaN\ndfUnique['polarity'][dfUnique.sentiment == 0.5] = \"3\"\ndfUnique['polarity'][dfUnique.sentiment == 1] = \"1\"\ndfUnique['polarity'][dfUnique.sentiment == 0] = \"2\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Assign the sentiment to each extracted phrases\ncount the number of tweets, a phrase has occurred in positive, negative and neutral context. Assign the most occurred sentiment to the phrase"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\n#find the highest occurring sentiment corresponding to each tweet\ndef find_mode(a):\n    b = Counter(a).most_common(3)\n    mode = []; c_max = 0\n    for a,c in b:\n        if c>c_max:\n            c_max = c\n        if c_max == c:\n            mode.append(a)  \n    print(mode)\n    mode.sort()\n    print(mode)\n    \n    ## if mode is 3&2 i.e. neutral and negative, assign the overall sentiment for that phrase as negative, \n    ## if mode is 3&1 i.e. neutral and positive, assign the overall sentiment for that phrase as positive,\n    ## if mode is 2&1 i.e. negative and positive, assign the overall sentiment for that phrase as neutal, \n    ## if mode is 3&2&1 i.e. negative, positive and neutral, assign the overall sentiment for that phrase as neutral\n    \n    if len(mode) == 1:\n        return mode[0]\n    \n    elif (len(mode) == 2) & (mode[1]=='3'):\n        return mode[0]\n    else:\n        return 3\n    \n#1=>+ve 2=>-ve 3=>Neutral\nnarrative['expression'] = -1\ndfUnique = dfUnique.reset_index(drop = True)\nfor i in clusters:\n    tweets = dfUnique['fully_cleaned_tweet'][dfUnique.cl_num == i]\n    abstracts = narrative['abstraction'][narrative.cl_num == i] \n    for abst in abstracts:\n        sent = []\n        for tweet, polarity in zip(dfUnique['fully_cleaned_tweet'][dfUnique.cl_num == i], dfUnique['polarity'][dfUnique.cl_num == i]):\n            if abst in tweet:\n                sent = np.append(sent, polarity)\n        \n        \n        if len(sent)!=0:\n            ## if mode is 3&2-2, 3&1-1, 2&1-3, 3&2&1 - 3\n            senti = find_mode(sent)\n            if senti == '2':\n                sent_value = \"Negative\"\n            elif senti == '1':\n                sent_value = \"Positive\"\n            else:\n                sent_value = \"Neutral\"\n            narrative['expression'][(narrative.abstraction == abst) & (narrative.cl_num == i)] = sent_value\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"narrative","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the sentiments\nimport seaborn as sns\nsns.countplot(narrative['expression'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install openpyxl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import ExcelWriter\n\n#Save the narratives in an excel file \n\nwriter = pd.ExcelWriter('narrative.xlsx')\nfor i in clusters:\n    df1 = pd.DataFrame(dfUnique[['tweet','freq']][dfUnique.cl_num == i]).sort_values(['freq'], ascending = [0])\n    df1 = pd.DataFrame({'tweet': dfUnique['tweet'][dfUnique.cl_num == i], 'freq': dfUnique['freq'][dfUnique.cl_num == i]}) \n    df1 = df1.sort_values(['freq'], ascending = [0]) \n\n    df2 = pd.DataFrame({ 'abstraction': narrative['abstraction'][narrative.cl_num == i], 'expression': narrative['expression'][narrative.cl_num == i]})\n    df3 = pd.DataFrame({'abstraction': (len(df1)-len(df2))*['-'], 'expression': (len(df1)-len(df2))*['-']})\n    df2 = df2.append(df3)\n\n    df1 = df1.reset_index(drop=True)\n    df2 = df2.reset_index(drop=True)\n    df1['abstraction'] = df2['abstraction']\n    df1['expression'] = df2['expression']\n\n    df1.to_excel(writer,'narrative_cluster'+str(i))\n\nwriter.save()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}